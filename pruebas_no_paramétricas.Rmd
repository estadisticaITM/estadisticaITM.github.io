---
title: "Pruebas no paramétricas"
output:
  html_document:
    toc: true
    toc_depth: 5
 ---

Gran parte de los procedimientos descritos anteriormente de pruebas de hipótesis, consideran que las observaciones provienen de poblaciones normales. En muchas situaciones este supuesto no se cumple. Una alternativa a la no normalidad de los datos, es proceder a implementar pruebas no paramétricas.

# Pruebas de normalidad

La hipotesis nula y alternativa de normalidad son las siguientes:

 $$H_0:  \quad Los\quad datos\quad se\quad distribuyen \quad normal$$
 
$$H_1:  \quad Los\quad datos\quad no \quad se\quad distribuyen \quad normal$$
Existen diferentes pruebas para evaluar la normalidad, todas son de fácil implementación en R.


## Test de Kolmogorov-Smirnov K-S

Se emplea para saber si una distribución de probabilidad acumulada difiere de una distribución hipotética, por lo general la distribución normal, la uniforme, la de Poisson o la exponencial. Es decir permite contrastar si un conjunto de datos muestrales proviene de un tipo de distribución. 



**Estadístico**

Cuando K-S se aplica para contrastar la hipótesis de normalidad de la población, el estadístico de prueba es la máxima diferencia entre las funciones de distribución de probabilidad muestral y la teórica:

$$D=max|F_n-F_0(x)|$$

Siendo $F_n(x)$ la función de distribución muestral y $F_0(x)$ es la función teórica (normal) especificada en la hipotesis nula $H_0$


**Pruebas de hipótesis**

$H_0:$ Los datos analizados siguen una distribución M.


$H_1:$ Los datos analizados no siguen una distribución M



**Ejemplo probando la distribución exponencial**




Celia quiere medir el tiempo de atención a los usuarios. Se seleccionaron 20 personas y los tiempos de atención en minutos.



```{r}
require(car)
tiempo<-c(3.69, 39.50,  4.43,  2.70,  9.11, 10.21, 10.44,  2.57,  5.68,  0.80,
          12.63,  2.35, 25.47, 8.07,  0.96,  0.21, 12.06, 10.79,  6.58, 13.06)

par(mfrow=c(1,3))
hist(tiempo, xlab = "Tiempo", ylab = "Frecuencia", las=1, main = "", col = "gray")
qqPlot(tiempo, col = "gray", ylab="Tiempo")
plot(density(tiempo), xlab = "Tiempo", ylab = "Densidad", las=1, main = "")


```

Se procede a revisar el ajuste con respecto a una distribución exponencial con un α=0.05

Sea X el tiempo entre llegadas a Celia Express.

$$H_0:X∼exp$$

$$H_1:X≁exp$$
la siguiente función ayuda a estimar los parámetros del modelo

```{r}
library(MASS)
Ajustex <- fitdistr(tiempo,"exponential")
Ajustex

Ks<- ks.test(tiempo, "pexp", rate=Ajustex$estimate[1])
Ks

```

Según las pruebas realizadas, no se rechaza la hipótesis nula y por tanto, se asume la distribución exponencial.


**Ejemplo comparando dos distribuciones**

x y y provienen de la misma distribución

$$H_0:X∼y$$

$$H_1:X≁y$$

```{r}

x <- rnorm(50)
y <- runif(30)

par(mfrow=c(1,2))
hist(x)
hist(y)
ks.test(x, y)

```





## Test de Shapiro-Wilk

También se usa para contrastar normalidad de un conjunto de datos, cuando la muestra es como máximo de tamaño 50. Es una alternativa más potente que la prueba de K-S. 

Para efectuarla se calcula la media y la varianza muestral $S^2$, y se ordenan las observaciones de menor a mayor, luego se calculan las diferencias entre: el primero y el último; el segundo y el penúltimo; el tercero y el antepenúltimo, etc. y se corrigen con unos coeficientes tabulados por Shapiro-Wilk. 


El estadístico de prueba es:

$$W=\frac{D^2}{nS^2}$$


donde D es la suma de las diferencias corregidas.


**Ejemplo en R probando normalidad**


Se necesita verificar si es correcto suponer que el volumen de llenado (en onzas) de una máquina dispensadora de jugos sigue una distribución normal, por lo que se toman 25 botellas de forma aleatoria. Los datos del volumen de llenado obtenidos de la muestra se encuentran almacenados en el vector volumen.

**Hipótesis**

$H_0:$ el volumen de llenado (en onzas) sigue una distribución normal.

$H_1:$ el volumen de llenado (en onzas) no sigue una distribución normal.

Nivel de significancia: 0.05 (Hipotético).

**Analisis descriptivo**


```{r}

volumen <-c(8.39,12.14, 11.80,12.04,7.34,12.62,11.51,12.47,11.08,14.32,11.33,11.56, 12.79,11.72,12.84,11.73,12.1, 11.88,11.95,10.84,11.79,13.21,12.56,12.55,12.80)

mean(volumen)
sd(volumen)

require(car)
library(MASS)
par(mfrow=c(1,3))
hist(volumen, xlab = "Volumen de llenado", ylab = "Frecuencia", las=1, main = "", col = "gray")

plot(density(volumen), xlab = "Volumen de llenado", ylab = "Densidad", las=1, main = "")

qqPlot(volumen, xlab="Cuantiles teóricos", ylab="Cuantiles muestrales", las=1,main="")

ks.test(volumen, "pnorm", mean =11.81, sd=1.4)

shapiro.test(volumen)

```

**Ejemplo en R**

Se mide las concentraciones de cortisol en dos grupos de mujeres antes de dar a luz. Al grupo 1 se le practicó una cesárea de urgencias después de inducido el parto. Las del grupo 2, dieron a luz mediante operación cesárea o vía vaginal después de presentarse el trabajo de parto expontáneamente.

1. Verifique normalidad en los conjuntos de datos usando α = 0.05.

2. Verifique la igualdad de varianzas entre los grupos de datos

3. Según la prueba de varianzas, que prueba recomienda para comparar las medias entre los grupos de madres



```{r echo=TRUE}

###Ingresamos los datos como vectores de los dos grupos de madres

grupo1=c(411,466,432,409,381,363,449,483,438,381)
grupo2=c(584,553,516,688,650,590,574,700,831,688,478,689)         


## Prueba de normalidad
shapiro.test(grupo1)
shapiro.test(grupo2)

#prueba para saber si provienen de la misma distribución 
ks.test(grupo1,grupo2)

## Prueba de igualdad entre varinzas
var.test(grupo1,grupo2)

## prueba de diferencias entre medias
t.test (grupo1,grupo2,paired=FALSE,conf.level=0.95)


         
```

$$H_0:grupo1∼grupo2$$

$$H_1:grupo1≁grupo2$$



# Prueba no paramétrica para la diferencias entre dos poblaciones

## Prueba de Mann-Whitney

- Es una prueba no paramétrica para identificar **diferencias entre dos poblaciones** basadas en el análisis de dos muestras independientes. 
 
- Se usa cuando los conjuntos de datos no cumplen los requisitos de la prueba de t-Student y la normal, (normalidad de los datos y tamaño de muestra mayor a 30)

- Para realizar la prueba las observaciones de las dos muestras deben estar en orden ascendente.

- También se conoce con otros nombres: Mann–Whitney–Wilcoxon, Wilcoxon rank-sum test y Wilcoxon–Mann–Whitney. Por ello está disponible en R por medio de la función wilcox.test.

- La prueba se basa en una comparación de cada observación de una muestra  $x_i$ con cada observación en la segunda muestra $y_i$ Si las muestras tienen la misma mediana, entones cada observación tiene un 50% de probabilidad de ser mayor o menor que la observación correspondiente de la otra muestra. Por tanto plantea las hipotesis:

$$H_o:P(x_i>y_i)=0.5$$
$$H_1: P(x_i>y_i) \neq 0.5$$

**Ejemplo Prueba de wilcoxon en R**

Los datos siguientes datos corresponden a constantes de permeabilidad de la membrana chorioamnion en humanos (una membrana placentaria) medida a las 12 y 26 semanas de edad gestacional.


1. realice una prueba de normalidad para verificar la normalidad en los conjuntos de datos

2. Asumiendo que los datos no son normales, pruebe a un nivel de significancia de α = 0.05 para chequear la alternativa, mayor permeabilidad de la membrana chorioamnion para el inicio del embarazo.

```{r echo=TRUE}

x=c(0.80, 0.83, 1.89, 1.04, 1.45, 1.38, 1.91, 1.64, 0.73, 1.46)
y=c(1.15, 0.88, 0.90, 0.74, 1.21)

# prueba no parametrica dewilcoxon
wilcox.test(x,y, alternative = "g")

##PRUEBA DE NORMALIDAD
shapiro.test(x)
shapiro.test(y)


```

### Prueba de bondad de ajuste para datos discretos

Para estimar los parámetros de una distribución de probabilidad discreta (poisson y bonimial), se requiere la función goodfit del paquete vcd. Esta función también realiza la prueba de bondad y ajuste y sus argumentos son: variable de interés, tipo de distribución y método. Se usará el test de Chi-cuadradado a través del argumento “MinChisq”


En el supermercado doña Celia, se está estudiando el comportamiento del número de personas que llegan cada hora. Se analizaron 20 horas, cuyos datos se consignan a continuación:


**Análisis exploratorio**
Se analiza los gráficos para determinar una distribución hipotética.

```{r}

personas<-c(13, 14, 14, 19, 17, 14, 13,  9, 16, 16,13, 13, 15, 13,  7, 14, 14, 13, 20, 15)

par(mfrow=c(1,2))
hist(personas, xlab = "personas", ylab = "Frecuencia", las=1, main = "", col = "gray")
plot(density(personas), xlab = "personas", ylab = "Densidad", las=1, main = "")

```

En este caso, la variable de interés registra un número de eventos por unidad de tiempo, por lo que se sugiere analizar el ajuste a una distribución poisson. Se muestra la respectiva prueba de hipótesis. Sea X el número de clientes que visitan Celia Express.

$H_0:$X∼Poisson

$H_1:$X≁Poisson

Se realizará el análisis con un α=0.01

```{r}
require(vcd)
gf<-goodfit(personas, type = "poisson", method = "MinChisq")
gf$par

summary(gf)
```



### Prueba t-Student en R

Los siguientes datos corresponden al rendimiento por hectárea de cierta nueva variedad de trigo, medido en 9 lotes experimentales: 

3,35; 3,92; 4,26; 3,36; 3,72; 4,19, 3,42; 4,38; 4,5.3

1. Realice una prueba de normalidad sobre el vector de datos

2. Construya un intervalo de confianza al 90% de confianza para el rendimiento promedio de la nueva variedad de trigo si suponemos que el rendimiento por hectárea se distribuye aproximadamente normal.

3. Para la salida de R construya una prueba de hipótesis

4. Verifique la hipótesis que el rendimiento medio es mayor que 4



```{r echo=TRUE}

#vector de datos
T<-c(3.35,3.92,4.26,3.36,3.72,4.19,3.42,4.38,4.5)

#Prueba de normalidad sobre el vector T
shapiro.test(T)

#Tabulación de datos
table(T)

#histograma de los datos
hist(T,breaks=3,col=4)

#gráfico de densidad de dtos
plot(density(T))

##prueba t de dos colas
t.test(T,conf.level=0.90)

##prueba t de cola superior
t.test(T,conf.level=0.90,alternative="greater",mu=4)

```


# Test de independencia

Dos variables aleatorias X e Y son llamadas independientes si la distribución de probabilidad de una de las variables no es afectada por la presencia de la otra.
Asuma que $Oij$ es la frecuencia observada de eventos que pertenecen a ambos, la i−ésima categoría de X y la j−ésima categoría de Y. Además, suponga que eij son las correspondientes frecuencias esperadas si X e Y son independientes.

**Prueba de hipótesis**
$$H_0:  \quad Los\quad datos\quad son \quad independientes$$
$$H_1:  \quad Los\quad datos\quad no \quad son \quad independientes$$


**Estadístico:**

$$ \large \chi^2=\sum_{i,j}^n \frac {(o_{ij}-e_{ij})^2}{e_{ij}}$$



## Ejemplo en R:

Se usan los datos de la base survey de la librería MASS de R, que corresponden a 237 observaciones provenientes de una encuesta a estudiantes de estadística de una Universidad en Australia.

1. Valide la hipótesis de si el hábito de fumar es independiente del nivel de ejercicios de los estudiantes usando un nivel de significancia del 0.05.

```{r }
## Para inst lar librerías use

# install.packages("MASS")

## Para llamar la librería
library(MASS)

##la base de datos total corresponde a 
survey

##Para ver el encabezado de los datos
head(survey)


##Para ver la ultima parte de los datos
tail(survey)

## Para obtener un resumen de los datos
summary(survey)


## se usan las variables
## FUMA (Smoke) con los niveles: Heavy, Regul, Occas y Never 
## EJERCICIO (Exer) con los niveles: Freq, Some, y None
##se tabulan

tbl=table(survey$Smoke,survey$Exer)
tbl

chisq.test(tbl)


#Note que aparece un mensaje de alerta. Esto es debido a que en algunas celdas las
#frecuencias son muy pequeñas. Podemos solucionar esto agrupando algunas columnas.

ctbl = cbind(tbl[,"Freq"], tbl[,"None"] + tbl[,"Some"])
ctbl

chisq.test(ctbl)


```

