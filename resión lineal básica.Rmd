

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Regresión lineal 

Permite establecer asociaciones entre variables de interés, entre las cuáles la relación usual no es necesariamente de causa efecto.

El objetivo es obtener estimaciones razonables de Y para distintos valores de X a partir de una muestra de n pares de valores (x1, y1), . . . ,(xn, yn).

##Antes de hacer un modelo de regresión lineal
Se deben identificar observaciones extremas, alejadas hacia valores muy grandes o pequeños comparadas con el resto de valores, que puedan influenciar el ajuste de regresión. Para ello se recurre a un análisis descriptivo a través de boxplots ó diagramas de puntos.

## Modelo de regresión lineal básico

El modelo más simple de regresión corresponde a: 
$$\Large y_i=\beta_0 +\beta_1 X_i+\varepsilon_i$$
Donde:

$\Large y_i$Es la variable respuesta o dependiente para la i-ésima observación\


$\Large \beta_0$ Intercepto\


$\Large\beta_1$ Pendiente\


$\Large X_i$ Variable predictora independiente para la i-ésima observación\


$\Large \varepsilon_i$ Error aleatorio para la i-ésima observación\


$$\Large \varepsilon_i \sim N (0,\sigma^2)$$


## Objetivos de la regresión lineal

- Construir un modelo que describa el efecto o relación entre una variable X sobre otra variable Y. 

- Obtener estimaciones puntuales de los parámetros de dicho modelo. 

- Estimar el valor promedio de Y para un valor de X 

- Predecir futuros de la variable respuesta Y

## Algunos ejemplos

- Estudiar cómo influye la estatura del padre sobre la estatura del hijo. 

- Estimar el precio de una vivienda en función de su área.

- Aproximar la calificación obtenida en una materia según el numero de horas de estudio semanal. 

##Diagrama de dispersión

Diagrama matemático que utiliza las coordenadas cartesianas para mostrar los valores de dos variables para un conjunto de datos.


```{r pressure}
plot(pressure)

```



#Medidas de dependencia lineal

##Covarianza

La covarianza indica el grado de variación conjunta de dos variables aleatorias respecto a sus medias

$$\Large cov(x,y)=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{(n-1)}$$
- Si hay relación lineal positiva, la covarianza será positiva y grande.

- Si hay relación lineal negativa, la covarianza será negativa y grande en valor absoluto.

- Si no hay relación entre las variables la covarianza será próxima a cero.

- La covarianza depende de las unidades de medida de las variables.

##Coeficiente de correlación

Indica la fuerza y la dirección de una relación lineal y proporcionalidad entre dos variables cuantitativas estadísticas.


$$\Large cor(x,y)=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}
{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2\sum_{i=1}^{n}(y_i-\bar{y})^2 }}$$



##Características del coeficiente de correlación

- Rango entre -1 y 1

- Valores cercanos a -1 la relación es fuertemente negativa.

- Valores cercanos a 1 la relación es fuertemente positiva.

- Valores cercanos a 0 la relación es débil, es decir no hay una relación lineal


```{r fig.asp=0.5, fig.align='right', echo=FALSE}
library(png)
library(grid)
IM <- readPNG("corre.png")
grid.raster(IM)
```


## Medida de bondad de ajuste R^2

Mide la proporción de la variabilidad total observada en la respuesta que es explicada por la asociación lineal.
Por ser una proporción, esta cantidad varía entre 0 y 1, siendo igual a 0 cuando todos los coeficientes de regresión ajustados son iguales a cero, y es igual a 1 si todas las observaciones caen sobre la superficie de regresión ajustada. Definido como:


$$\Large R^2=1-\frac{SSE}{SST}=1-\frac{\sum_{i=1}^n (y_i-\hat{y_i})^2}{\sum_{i=1}^n (y_i-\bar{y_i})^2}$$
Donde

SSE: es la suma de cuadrados del error
SST: suma de cuadrados totales

## Error de pronóstico medio MAPE
Es importante evaluar la capacidad predictiva del modelo. En el trabajo aplicado, comparando los valores observados con los pronosticados, para lo cual se cuenta con el promedio del error porcentual aboluto. Dado por la fórmula:

$$\Large MAPE=\frac{\sum_{i=1}^n |\frac{y_i-\hat{y_i}}{y_i}|}{n}$$




##Estimador de mínimos cuadrados
Gauss propuso en 1809 el método de mínimos cuadrados para obtener los valores $\hat{\beta_0}, \hat {\beta_1}$ que mejor se ajustan a los datos:

$$\Large y_i=\beta_0+\beta_1x_i+\varepsilon_i$$

El método consiste en minimizar la suma de los cuadrados de las distancias verticales entre los datos y las estimaciones, es decir, minimizar la suma de los residuos al cuadrado:



$$\sum_{i=1}^n(y_i-\hat{y_i})^2=
\sum_{i=1}^n (y_i-(\hat{\beta_0}+ \hat{\beta_1}x_i))^2$$




el resultado que se obtiene es:

$$\Large \hat{\beta_1}=\frac{S_{xy}}{S_{xx}}=\frac{cov(x,y)}{S_{xx}}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$

A las cantidades $\Large S_{xx}$ y $\Large S_{xy}$ se les conoce como suma corregida de cuadrados y suma corregida de productos cruzados de x y y, respectivamente
$$\Large \hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$$

```{r fig.asp=0.5, fig.align='right', echo=FALSE}
library(png)
library(grid)
IM3 <- readPNG("mc.png")
grid.raster(IM3)
```



##Residuales

La diferencia de cada valor $y_i$ de la variable respuesta y su estimación $\hat{y_i}$ se llama residuo.
$$\Large \varepsilon_i= y_i- \hat{y_i}$$



```{r fig.asp=0.5, fig.align='right', echo=FALSE}
library(png)
library(grid)
IM4 <- readPNG("img4.png")
grid.raster(IM4)
```




##Regresión lineal múltiple

Considere el caso en el cual se desea modelar la variabilidad total de una variable respuesta de interés, en función de relaciones lineales con dos o más variables predictoras, formuladas simultáneamente en un único modelo. Suponemos en principio que las variables predictoras guardan poca asociación lineal entre sí, es decir, cada variable predictora aporta información independiente de las demás predictoras presentes en el modelo (hasta cierto grado, la información aportada por cada una no es redundante). La ecuación del modelo de regresión en este caso es:

$$\Large y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+...+\beta_kx_{ik}\varepsilon_i$$

## Tipos de variables y de efectos en los modelos

Las variables predictoras pueden ser:
 Cuantitativas, caso en el cual se supone se miden sin error (o el error es despreciable).

 Cualitativas o categóricas, en este caso su manejo en el modelo se realiza a través de la definición de variables  indicadoras, las cuales toman valores de 0 ó 1. 

Por ejemplo, suponga que en un modelo de regresión para el gasto mensual por familia en actividades recreativas, se tiene entre las variables predictoras el estrato socioeconómico, definido en cinco niveles, luego, para cada nivel se define una variable indicadora de la siguiente forma:


Estrato 1:
$$ \Large

I_1 =\left\lbrace \begin{array}{rcl}
            {1\quad familia \quad estrato \quad 1}
         \\
            {0 \quad En \quad otro \quad caso  }
         \end{array}  \right. 
         
$$
Estrato 2

$$ \Large
         
         I_2 =\left\lbrace   \begin{array}{rcl}
            
            {1\quad familia \quad estrato \quad 2}
         \\
            
            {0 \quad En \quad otro \quad caso  }
         \end{array}  \right.
         
         $$
Estrato 3

$$ \Large

         I_3 =\left\lbrace   \begin{array}{rcl}
            
            {1\quad familia \quad estrato \quad 3}
         \\
            
            {0 \quad En \quad otro \quad caso  }
         \end{array}  \right. 
         
  $$
  
  Estrato 4
  $$
    \Large     
         I_4 =\left\lbrace   \begin{array}{rcl}
            
            {1\quad familia \quad estrato \quad 4}
         \\
            
            {0 \quad En \quad otro \quad caso  }
         \end{array}  \right.
         
         $$

En general, una variable cualitativa con c clases se representa mediante c -1 variables indicadoras, puesto que cuando en una observación dada, todas las c -1 primeras indicadoras son iguales a cero, entonces la variable cualitativa se haya en su última clase. En el ejemplo anterior basta definir las primeras cuatro indicadoras.

Ejemplo:
La siguiente base de datos relaciona 7 medidas del crecimiento de 5 arboles en el tiempo en meses y el diámetro en mm.

```{r}

head (Orange)



```

El diagrama de dispersión simple es:

```{r}
plot(Orange$age,Orange$circumference,lwd=3)
```

El modelo de regresión lineal simple corresponde a:

```{r}

model=lm(Orange$circumference~Orange$age)
summary(model)



```





El diagrama de dispersión discriminando por los niveles de la variables factor es:

```{r}
plot(Orange$age,Orange$circumference,col=Orange$Tree,lwd=3)

```

El modelo de regresión lineal con factores corresponde a 
```{r}
model=lm(Orange$circumference~Orange$age+as.factor(Orange$Tree))
```


