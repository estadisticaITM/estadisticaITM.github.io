

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Regresión lineal 

Permite establecer asociaciones entre variables de interés, entre las cuáles la relación usual no es necesariamente de causa efecto.

El objetivo es obtener estimaciones razonables de Y para distintos valores de X a partir de una muestra de n pares de valores (x1, y1), . . . ,(xn, yn).

##Antes de hacer un modelo de regresión lineal
Se deben identificar observaciones extremas, alejadas hacia valores muy grandes o pequeños comparadas con el resto de valores, que puedan influenciar el ajuste de regresión. Para ello se recurre a un análisis descriptivo a través de boxplots ó diagramas de puntos.

## Modelo de regresión lineal básico

El modelo más simple de regresión corresponde a: 
$$\Large y_i=\beta_0 +\beta_1 X_i+\varepsilon_i$$
Donde:

$\Large y_i$Es la variable respuesta o dependiente para la i-ésima observación\


$\Large \beta_0$ Intercepto\


$\Large \beta_1$ Pendiente\


$\Large X_i$ Variable predictora independiente para la i-ésima observación\


$\Large \varepsilon_i$ Error aleatorio para la i-ésima observación\


$$\Large \varepsilon_i \sim N (0,\sigma^2)$$


## Objetivos de la regresión lineal

- Construir un modelo que describa el efecto o relación entre una variable X sobre otra variable Y. 

- Obtener estimaciones puntuales de los parámetros de dicho modelo. 

- Estimar el valor promedio de Y para un valor de X 

- Predecir futuros de la variable respuesta Y

## Algunos ejemplos

- Estudiar cómo influye la estatura del padre sobre la estatura del hijo. 

- Estimar el precio de una vivienda en función de su área.

- Aproximar la calificación obtenida en una materia según el numero de horas de estudio semanal. 

##Diagrama de dispersión

Diagrama matemático que utiliza las coordenadas cartesianas para mostrar los valores de dos variables para un conjunto de datos.


```{r pressure}
plot(pressure)

```



#Medidas de dependencia lineal

##Covarianza

La covarianza indica el grado de variación conjunta de dos variables aleatorias respecto a sus medias

$$\Large cov(x,y)=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{(n-1)}$$
- Si hay relación lineal positiva, la covarianza será positiva y grande.

- Si hay relación lineal negativa, la covarianza será negativa y grande en valor absoluto.

- Si no hay relación entre las variables la covarianza será próxima a cero.

- La covarianza depende de las unidades de medida de las variables.

##Coeficiente de correlación

Indica la fuerza y la dirección de una relación lineal y proporcionalidad entre dos variables cuantitativas estadísticas.


$$\Large cor(x,y)=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}
{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2\sum_{i=1}^{n}(y_i-\bar{y})^2 }}$$


##Características del coeficiente de correlación

- Rango entre -1 y 1

- Valores cercanos a -1 la relación es fuertemente negativa.

- Valores cercanos a 1 la relación es fuertemente positiva.

- Valores cercanos a 0 la relación es débil, es decir no hay una relación lineal


```{r fig.asp=0.3, fig.align='right', echo=FALSE}
library(png)
library(grid)
IM <- readPNG("corre.png")
grid.raster(IM)
```

## Medida de bondad de ajuste R^2

Mide la proporción de la variabilidad total observada en la respuesta que es explicada por la asociación lineal.
Por ser una proporción, esta cantidad varía entre 0 y 1, siendo igual a 0 cuando todos los coeficientes de regresión ajustados son iguales a cero, y es igual a 1 si todas las observaciones caen sobre la superficie de regresión ajustada. Definido como:


$$\Large R^2=1-\frac{SSE}{SST}=1-\frac{\sum_{i=1}^n (y_i-\hat{y_i})^2}{\sum_{i=1}^n (y_i-\bar{y_i})^2}$$
Donde

SSE: es la suma de cuadrados del error
SST: suma de cuadrados totales

## Error de pronóstico medio MAPE
Es importante evaluar la capacidad predictiva del modelo. En el trabajo aplicado, comparando los valores observados con los pronosticados, para lo cual se cuenta con el promedio del error porcentual aboluto. Dado por la fórmula:

$$\Large MAPE=\frac{\sum_{i=1}^n |\frac{y_i-\hat{y_i}}{y_i}|}{n}$$



##Estimador de mínimos cuadrados
Gauss propuso en 1809 el método de mínimos cuadrados para obtener los valores $\hat{\beta_0}, \hat {\beta_1}$ que mejor se ajustan a los datos:

$$\Large y_i=\beta_0+\beta_1x_i+\varepsilon_i$$

El método consiste en minimizar la suma de los cuadrados de las distancias verticales entre los datos y las estimaciones, es decir, minimizar la suma de los residuos al cuadrado:



$$\Large \sum_{i=1}^n(y_i-\hat{y_i})^2=
\sum_{i=1}^n (y_i-(\hat{\beta_0}+ \hat{\beta_1}x_i))^2$$




el resultado que se obtiene es:

$$\Large \hat{\beta_1}=\frac{S_{xy}}{S_{xx}}=\frac{cov(x,y)}{S_{xx}}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$

A las cantidades $\Large S_{xx}$ y $\Large S_{xy}$ se les conoce como suma corregida de cuadrados y suma corregida de productos cruzados de x y y, respectivamente
$$\Large \hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}$$

```{r fig.asp=0.3, fig.align='right', echo=FALSE}
library(png)
library(grid)
IM3 <- readPNG("mc.png")
grid.raster(IM3)
```


##Residuales

La diferencia de cada valor $y_i$ de la variable respuesta y su estimación $\hat{y_i}$ se llama residuo.
$$\Large e_i= y_i- \hat{y_i}$$




```{r fig.asp=0.3, fig.align='right', echo=FALSE}
library(png)
library(grid)
IM4 <- readPNG("img4.png")
grid.raster(IM4)
```




La varianza de los términos de error $\varepsilon_i$, es decir, $V[\varepsilon_i]=\sigma^2$, da un indicador de la variabilidad de las distribuciones de probabilidad de Y para los distintos valores de X. En este caso la suma cuadrática de errores o residuales es:

$$\Large SSE=\sum_{i=1}^{n}(y_i-\hat{y_i})^2=\sum_{i=1}^n e^2_i$$
La SSE tiene asociada n−2 grados de libertad (gl), pues se pierden 2 (gl) al estimar $\beta_0$ y $\beta_1$, para obtener a $\hat{y_i}$, de lo anterior se obtiene la media cuadática de errores dada por:

$$\Large MSE=\frac{SSE}{n-2}=\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{n-2}=\frac{\sum_{i=1}^n e^2_i}{n-2}$$

Se puede demostrar que MSE, es un estimador insesgado de $\Large \sigma^2$ para el modelo de RLS, es decir que:

$$\Large \sigma^2=MSE$$
Además
$$\Large E(MSE)=\sigma$$
###Suma total de cuadrados

La medida de variación de y alrededor de la la media muestral $\bar{y}$ es:
$$\Large SST=\sum_{i=1}^n(Y_i-\bar{Y})^2$$
## Suma cuadrática de regresión
La diferencia entre la SST y la SSE se denota por:

$$\Large SSR=\sum_{i=1}^n(\hat{Y_i}-\bar{Y})^2$$
La SSR es una medida de la parte de la variabilidad de las observaciones $y_i$, la cual está asociada con la línea de regresión ajustada.

De lo anterior, se obtiene la identidad fundamental del análisis de varianza, la cual está dada por:

$$\Large SST = SSR + SSE$$

Aquí,
SST: Variabilidad muestral total y tiene n−1 grados de libertad.

SSR: Variabilidad explicada por el modelo o por las variables regresoras X y tiene 1 grado de libertad.

SSE: Variabilidad no explicada por el modelo o error y tiene n−2 grados de libertad.

## Medias cuadráticas 
Las medias cuadráticas se obtienen, como las SS divididas por sus respectivos grados de libertad, es decir que:

$\Large MST=\frac{SST}{n-1}$: Cuadrado medio total

$\Large MSR=\frac{SSR}{1}$:Cuadrado medio de la regresión

$\Large MSE=\frac{SSE}{n-2}$: Cuadrado medio del error



##Tabla de resumen del análisis de varianza

|Factor   |Grados de libertad|Suma de cuadrados|Medias cuadráticas|Estadístico|
|---------|------------------|-----------------|------------------|-----------|
|Regresión|1                 |SSR              |MSR               |Fc         |
|Error    |n-2               |SSE              |MSE               |           |
|Total    |n-1               |SST              |MST               |           |


##Prueba de significancia de la regresión

Considere las siguientes afirmaciones, las cuales son llamadas pruebas de hipótesis o prueba de significancia de la regresión:

Hipótesis nula:   $\LargeH_0: \beta_i=0$ el parámetro no es significativo

Hipótesis alternativa:  $\Large H_1: \beta_i \not= 0$: el parámeto es significativo

El estadístico de prueba corresponde a 

$$\Large F_c=\frac{MSR}{MSE}\sim F_{(1,n-2)}$$

##Desición

Rechazar $\Large H_0$ si $\Large F_c>F_{(1,n-2)}$

##Valor p



##Estimación de la varianza
Para estimar la varianza de los errores $\sigma^2$, podemos utilizar el estimador máximo verosimil:


$$\Large {\hat{\sigma}}^2=\frac{\sum_{i=1}^n {e}_i^2}{n}$$


## Significado de $\beta_0$ y $\beta_1$

- $\Large \beta_0$ es el intercepto

- $\Large \beta_1$ es el valor de la pendiente, es decir que por cada unidad que aumente la variable independiente, la variable dependiente aumenta $\beta_1$ unidades


##Ejemplo
Los siguientes datos proveen las velocidades en metros por segundo y las distancias necesarias para frenar en metros

```{r echo = TRUE}
head(cars)

```

En este caso ¿Cuál es la variable dependiente e independiente?

## Diagrama de dispersión
```{r echo=TRUE}

plot(cars$speed,cars$dist)

```


## Modelo de regresión lineal

Para este modelo el modelo de regresión lineal es

$$\Large \hat{y}=-17.57+3.93x$$

Donde 

- X representa la velocidad en metros por segundo

- y es la distancia de frenado en metros

## Resultados del modelo en R

```{r}

plot(cars$speed,cars$dist)

md=lm(cars$dist~cars$speed)

summary(md)

j=fitted(md)

lines(cars$speed,j, col=2, lwd=2)

cor(cars$dist,cars$speed)

```

###anova del modelo

```{r}

md=lm(cars$dist~cars$speed)

anova(md)



```




## Modelo de regresión lineal simple en la calculadora

- como estimar un modelo de [regresión lineal](https://www.youtube.com/watch?v=4_WO31Dapv0) en la calculadora
- Dados los siguientes datos elabore 
1. Diagrama de dispersión
2. Ajuste del modelo lineal
3. Estime el valor de la correlación

##Validación de los supuestos

A través del análisis de residuales del modelo es posible detectar la linealidad entre las variables X e Y, la media cero, varianza constante, incorrelación, y normalidad.

###QQplot 
permite verificar la normalidad de los errores. Si es cierta la normalidad de los residuos, los puntos deben estar alineados con la diagonal.

```{r}

md=lm(cars$dist~cars$speed)
rr=residuals(md)
plot(md)

```

###Gráfico de los residuos versus los valores ajustados

Si los residuos están distribuidos alrededor del cero y el gráfico no presenta ninguna tendencia, entonces el modelo se considera adecuado. Note que si se observa una tendencia, estaríamos violando el supuesto de linealidad (lo cual sugiere alguna transformación), y si se observa una nube de puntos en forma de embudo, podemos tener problemas con el supuesto de homocedasticidad de varianzas.


##Regresión lineal múltiple

Considere el caso en el cual se desea modelar la variabilidad total de una variable respuesta de interés, en función de relaciones lineales con dos o más variables predictoras, formuladas simultáneamente en un único modelo. Suponemos en principio que las variables predictoras guardan poca asociación lineal entre sí, es decir, cada variable predictora aporta información independiente de las demás predictoras presentes en el modelo (hasta cierto grado, la información aportada por cada una no es redundante). La ecuación del modelo de regresión en este caso es:

$$\Large y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+...+\beta_kx_{ik}\varepsilon_i$$

## Tipos de variables y de efectos en los modelos

Las variables predictoras pueden ser:
 Cuantitativas, caso en el cual se supone se miden sin error (o el error es despreciable).

 Cualitativas o categóricas, en este caso su manejo en el modelo se realiza a través de la definición de variables  indicadoras, las cuales toman valores de 0 ó 1. 

Por ejemplo, suponga que en un modelo de regresión para el gasto mensual por familia en actividades recreativas, se tiene entre las variables predictoras el estrato socioeconómico, definido en cinco niveles, luego, para cada nivel se define una variable indicadora de la siguiente forma:


Estrato 1:
$$ \Large

I_1 =\left\lbrace \begin{array}{rcl}
            {1\quad familia \quad estrato \quad 1}
         \\
            {0 \quad En \quad otro \quad caso  }
         \end{array}  \right. 
         
$$
Estrato 2

$$ \Large
         
         I_2 =\left\lbrace   \begin{array}{rcl}
            
            {1\quad familia \quad estrato \quad 2}
         \\
            
            {0 \quad En \quad otro \quad caso  }
         \end{array}  \right.
         
         $$
Estrato 3

$$ \Large

         I_3 =\left\lbrace   \begin{array}{rcl}
            
            {1\quad familia \quad estrato \quad 3}
         \\
            
            {0 \quad En \quad otro \quad caso  }
         \end{array}  \right. 
         
  $$
  
  Estrato 4
  $$
    \Large     
         I_4 =\left\lbrace   \begin{array}{rcl}
            
            {1\quad familia \quad estrato \quad 4}
         \\
            
            {0 \quad En \quad otro \quad caso  }
         \end{array}  \right.
         
         $$

En general, una variable cualitativa con c clases se representa mediante c -1 variables indicadoras, puesto que cuando en una observación dada, todas las c -1 primeras indicadoras son iguales a cero, entonces la variable cualitativa se haya en su última clase. En el ejemplo anterior basta definir las primeras cuatro indicadoras.

Ejemplo:
La siguiente base de datos relaciona 7 medidas del crecimiento de 5 arboles en el tiempo en meses y el diámetro en mm.

```{r}

head (Orange)



```

El diagrama de dispersión simple es:

```{r}
plot(Orange$age,Orange$circumference,lwd=3)
```

El modelo de regresión lineal simple corresponde a:

```{r}

model=lm(Orange$circumference~Orange$age)
summary(model)



```





El diagrama de dispersión discriminando por los niveles de la variables factor es:

```{r}
plot(Orange$age,Orange$circumference,col=Orange$Tree,lwd=3)

```

El modelo de regresión lineal con factores corresponde a 
```{r}
model=lm(Orange$circumference~Orange$age+as.factor(Orange$Tree))
```


